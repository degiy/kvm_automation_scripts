
================================================================================

A/ Install physique 3 serveurs e1,e2,e3
  - provisionning SLI
  - reception par EEI
  - mise en baie EEI
  - cablage reseau EEI + cable direct entre e1 et e2
  - installation OS debian par EEI
  
10.200.10.81    ebrox e1
10.200.10.82    edino e2
10.200.10.83    edopi e3

# cat /etc/debian_version  : 9.6 (stretch de 2017)
4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux
QEMU emulator version 2.8.1(Debian 1:2.8+dfsg-6+deb9u5)

disques :

   8        0  292935982 sda
   8        1     524288 sda1 (/boot)
   8        2  292410670 sda2 (/lvm)

  --- Logical volume ---
  LV Path                /dev/vg0/swap
  LV Size                8.00 GiB
  --- Logical volume ---
  LV Path                /dev/vg0/root
  LV Size                1.00 GiB
  --- Logical volume ---
  LV Path                /dev/vg0/var
  LV Size                4.00 GiB
  --- Logical volume ---
  LV Path                /dev/vg0/usr
  LV Size                8.00 GiB
  --- Logical volume ---
  LV Path                /dev/vg0/kvm
  LV Size                200.00 GiB
  --- Logical volume ---
  LV Path                /dev/vg0/home
  LV Size                6.00 GiB

réseau :

2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master br0 state UP mode DEFAULT group default qlen 1000
3: eno2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000

3: eno2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether a0:d3:c1:fa:dc:61 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.2/24 brd 192.168.1.255 scope global eno2
    
3: eno2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether ac:16:2d:6e:bc:c5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.1/24 brd 192.168.1.255 scope global eno2

internet :

ping proxy.eei.cena.fr     
PING tahsa.eei.cena.fr (10.196.32.91)

search eei.cena.fr
nameserver 10.196.32.89
nameserver 10.196.32.91

================================================================================

B/ Recup ISO + systeme de base machines hote + partage

- centos 7.6 et fedora server 28 et debian 9.6
dans : /kvm/iso et montage dans /mnt/iso

- export

/kvm/iso/fedora_server_28.iso   /mnt/iso/f28    iso9660 ro,relatime
/kvm/iso/centos_76.iso  /mnt/iso/c76    iso9660 ro,relatime
/kvm/iso/debian-9.6.0-amd64-DVD-1.iso   /mnt/iso/d96    iso9660 ro,relatime

- montage nfs via réseau eei sur 3 autres serveurs de ces 2 repertoires

#systemctl status nfs-kernel-server
dans /etc/exports
/kvm/iso        e2(ro,async,no_subtree_check,no_root_squash) e3(ro,async,no_subtree_check,no_root_squash)
/mnt/iso/c76 192.168.220.0/24(ro,async,no_subtree_check,no_root_squash) e2(ro,async,no_subtree_check,no_root_squash) e3(ro,async,no_subtree_check,no_root_squash)
/mnt/iso/d96 192.168.220.0/24(ro,async,no_subtree_check,no_root_squash) e2(ro,async,no_subtree_check,no_root_squash) e3(ro,async,no_subtree_check,no_root_squash)
/mnt/iso/f28 192.168.220.0/24(ro,async,no_subtree_check,no_root_squash) e2(ro,async,no_subtree_check,no_root_squash) e3(ro,async,no_subtree_check,no_root_squash)
/kvm/t  e2(rw,sync,no_subtree_check,no_root_squash) e3(rw,sync,no_subtree_check,no_root_squash) 192.168.220.0/24(rw,sync,no_subtree_check,no_root_squash)

exportfs -a

- import

mkdir /mnt/iso
mkdir /kvm/iso
mkdir /kvm/t  

ds /etc/fstab

e1:/kvm/iso /kvm/iso nfs ro
e1:/mnt/iso /mnt/iso nfs ro
e1:/kvm/t /kvm/t nfs rw

mount -a

================================================================================

C/ LANs partagés

- ménage eno2 de nicolas
ip a del 192.168.1.1/24 dev eno2
ip a del fe80::ae16:2dff:fe6e:bcc5/64 dev eno2

nom     abr  @ip          nm  vlanid
-------+---+-------------+---+----+
admin   ADM 192.168.220.0 /24  220
interne INT 192.168.221.0 /24  221
ingres  ING 192.168.222.0 /24  222
esb     ESB 192.168.223.0 /24  223
storage STO 192.168.224.0 /24  224

- bridge des 2 côtés, ex de /etc/interfaces

iface eno2.220 inet manual
 vlan-raw-device eno2

auto br_adm
iface br_adm inet static
 address 192.168.220.2/24
 bridge_ports eno2.220
 bridge_stp on
 brdige_maxwait 10

voir script (gen_...pl)

DNS ? à voir

================================================================================

D/ creation VM générique (@IP + cle + montage commun)

- autorisation de montage iso via nfs via admin

paramètres :
- passage ks par ajout de fichier à l'initrd (injection)
- ip (passage via ks)
- serveur ntp
- accès à l'iso (ip + path par nfs)
- compte root accessible depuis les machines d'admin (post-install)


================================================================================
E/ creation des interfaces supplémentaires des vms

- voir script gen.pl et utilisation des commandes virsh atach et montage qemu d'image qcow
- config ansible par groupe (tout, par serveur hote et par fonction transverse)



================================================================================
F/ VM gluster fs

- backup des vms de e1 sur e3 (pour reprendre rapidement à partir de l'image qemu)
  attention à conserver le sparse mode sur la copie via nfs
    cp --sparse=always sa.img /kvm/backups/
    1133900 -rw------- 1 root root 2148073472 Dec 28 14:08 sa.img

- test sur noeuds ds et vlan cluster entre les ds : sa & sb
- échanges de clés ssh entre machines, et test ssh (via vlan storage)

- récup des rpms sur http://mirror.centos.org/centos/7/storage/x86_64/gluster-3.12/

yum install ./glusterfs-server-3.12.15-1.el7.x86_64.rpm ./glusterfs-api-3.12.15-1.el7.x86_64.rpm ./glusterfs-libs-3.12.15-1.el7.x86_64.rpm ./glusterfs-client-xlators-3.12.15-1.el7.x86_64.rpm ./glusterfs-fuse-3.12.15-1.el7.x86_64.rpm ./glusterfs-cli-3.12.15-1.el7.x86_64.rpm ./glusterfs-3.12.15-1.el7.x86_64.rpm userspace-rcu-0.10.0-3.el7.x86_64.rpm

sur chaque serveur :

systemctl enable glusterd
systemctl start glusterd
systemctl status glusterd
gluster peer probe 192.168.227.142
gluster peer probe 192.168.224.142
# sans dernière ligne pas moyen de se connecter de l'autre réseau
mkdir /gv0

sur le premier :

gluster volume create gv0 replica 2 192.168.224.141:/gv0 192.168.224.142:/gv0 force
gluster volume start gv0
gluster volume info

(pareil pour gv1)

sur les 2 (pour test) :

mkdir /mnt/gv0
mount -t glusterfs 192.168.227.141:/gv0 /mnt/gv0

[ tester que ça marche au reboot ]

- sur BA et BB
ds /etc/fstab
192.168.224.141:/gv0    /mnt/gv0        glusterfs       rw,backupvolfile-server=192.168.224.142 0 0
192.168.224.142:/gv1    /mnt/gv1        glusterfs       rw,backupvolfile-server=192.168.224.141 0 0

================================================================================
G/ HA Proxy sur LB
(pas urgent)

================================================================================
H/ IP relogeable sur DMZ + clustering actif/passif avec pacemaker
sur dmz a & b :

# install de pcs (pacemaker)
yum install pcs -y

# choix d'un mdp (h)
passwd hacluster

# activation service
systemctl enable pcsd.service
systemctl start pcsd.service

# creation du cluster sur dmz a & b (commande que sur a), tester auth avec options -u user -p pw
pcs cluster auth dmz_a_cluster_dmz dmz_b_cluster_dmz
pcs cluster setup --start --name dmz_cluster dmz_a_cluster_dmz dmz_b_cluster_dmz
pcs cluster enable --all

# disable stonith
pcs property set stonith-enabled=false

# ip virtuelle (pour l'instant sur admin...)
pcs resource create virtual_ip ocf:heartbeat:IPaddr2 ip=192.168.220.100 iflabel=ifcl op monitor interval="5s" timeout="10s"

================================================================================
H/ ajout d'une interface sortante sur le réseau EEI pour les machines DMZ A & B
trouver une ip dispo
map -sn 10.200.10.0/24 | grep 10.200.10 | sed -e 's/^.*10.200.10.//' -e 's/^).*$//' | sort -n
- > 84 et 85 en fixe et 86 en flottante

virsh attach-interface da bridge br0 --model virtio --config
virsh domiflist da | grep br0

=> 52:54:00:fe:ff:ce   da

virsh attach-interface db bridge br0 --model virtio --config
virsh domiflist db | grep br0

=> 52:54:00:94:da:d2   db

cd /etc/sysconfig/network-scripts/
cat ifcfg-eth2 | sed -e 's/eth2/eth3/g' -e 's/^HWADDR.*$/HWADDR=52:54:00:fe:ff:ce/' -e 's/^IPADDR=.*$/IPADDR="10.200.10.84"/' > ifcfg-eth3
cat ifcfg-eth2 | sed -e 's/eth2/eth3/g' -e 's/^HWADDR.*$/HWADDR=52:54:00:94:da:d2/' -e 's/^IPADDR=.*$/IPADDR="10.200.10.85"/' > ifcfg-eth3

pcs resource delete virtual_ip
pcs resource create virtual_ip ocf:heartbeat:IPaddr2 ip=10.200.10.86 iflabel=ifcl op monitor interval="5s" timeout="10s"


================================================================================
I1/ autour du load balancer (sur fa, fb)

Pour fa et fb (on reste en http simple : actif / actif)

yum install haproxy -y
systemctl enable haproxy

fichier /etc/rsyslog.d/haproxy.conf
$ModLoad imudp
$UDPServerAddress 127.0.0.1
$UDPServerRun 514

local2.*        /var/log/haproxy.log

fichier /etc/haproxy/haproxy.cfg
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     1000
    user        haproxy
    group       haproxy
    daemon

defaults
        mode    http

listen http-in
        bind load_balancer_a_intern:80
        log global
        option httplog
        balance roundrobin
        server f1 fuse_a_ingres:80 check
        server f2 fuse_b_ingres:80 check


systemctl restart rsyslog
systemctl start haproxy
tail -f /var/log/haproxy.log

================================================================================
I2/ reverse proxy (sur da, db)

# Pour da et db on est dans le cluster (un seul haproxy sur le noeud actif)

# recup du script pacemaker sur le site suivant ,
# à copier dans /usr/lib/ocf/resource.d/heartbeat/haproxy :

https://github.com/russki/cluster-agents

# mise à jour des fichiers sur le 2nd noeud
scp /usr/lib/ocf/resource.d/heartbeat/haproxy root@dbint:/usr/lib/ocf/resource.d/heartbeat
scp /etc/haproxy/haproxy.cfg root@dbint:/etc/haproxy/

creation du service 
pcs resource create reverse_proxy ocf:heartbeat:haproxy conffile=/etc/haproxy/haproxy.cfg op monitor interval="5s" timeout="10s"

# ordre de démarrage
pcs constraint order start virtual_ip then start reverse_proxy
pcs constraint order stop reverse_proxy then stop virtual_ip
# colocalisation
pcs constraint colocation add reverse_proxy virtual_ip INFINITY
pcs property set no-quorum-policy=ignore

================================================================================
I3/ PKI & TLS endpoint sur DMZ

=> generation du certificat

openssl req -newkey rsa:2048 -nodes -keyout da_rsa_2048.key -x509 -out da_cert_rsa_2048.crt -subj "/C=FR/ST=Toulouse/L=Mounede/O=DSNA/OU=DTI/CN=swimaman.asap.dsna.fr"
cat da_rsa_2048.key da_cert_rsa_2048.crt > /var/lib/haproxy/da_cert_rsa_2048.pem
chown haproxy.haproxy /var/lib/haproxy/da_cert_rsa_2048.pem

scp /var/lib/haproxy/da_cert_rsa_2048.pem root@dbint:/var/lib/haproxy/

/etc/haproxy/haproxy.cfg à adapter :

listen https-in
        bind 10.200.10.86:443 ssl crt /var/lib/haproxy/da_cert_rsa_2048.pem
        log global
        option httplog
        balance roundrobin
        server l1 load_balancer_a_intern:80 check
        server l2 load_balancer_b_intern:80 check
	
scp /etc/haproxy/haproxy.cfg dbint:/etc/haproxy/

=> verif
openssl x509 -text -noout -in da_cert_rsa_2048.crt 

curl https://10.200.10.86/index.html -k


================================================================================
J/ autour d'un serveur web bidon sur esb

yum install perl-HTTP-Daemon -y
mkdir site
cat > site/index.html
<html>
response fuse A
</html>
^D

cat > http_server.pl

#!/usr/bin/perl

use HTTP::Daemon;
use HTTP::Status;
 
my $d = HTTP::Daemon->new(LocalPort => 80) || die;
print "Please contact me at: <URL:", $d->url, ">\n";
while (my $c = $d->accept) {
    while (my $r = $c->get_request) {
        if ($r->method eq 'GET' ) {
            $c->send_file_response("/root/site/".$r->uri->path);
        }
        else {
            $c->send_error(RC_FORBIDDEN)
        }
    }
    $c->close;
    undef($c);
}
^D

chmod 755 ./http_server.pl 

en faire un service

cat > /etc/systemd/system/http_perl_server.service
[Unit]
Description=Serveur web en perl pour test esb
After=network-online.target
 
[Service]
Type=exec
 
User=root
Group=root
UMask=007
 
ExecStart=/root/http_server.pl
 
Restart=on-failure
 
# Configures the time to wait before service is stopped forcefully.
TimeoutStopSec=10
 
[Install]
WantedBy=multi-user.target
^D
systemctl enable http_perl_server
systemctl start http_perl_server
